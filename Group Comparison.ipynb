{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists you selected in Module 1. If the results from that pull were not to your liking, you are welcome to use the zipped data from the “Assignment Materials” section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e65f73",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c31a9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "from lexical_diversity import lex_div as ld\n",
    "import numpy as np\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"/s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, top_n_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "    # Place your Module 2 solution here\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = ld.ttr(tokens) # Simple TTR = len(Counter(text))/len(text)\n",
    "    num_characters = sum([len(i) for i in tokens])\n",
    "    \n",
    "    if verbose:        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(f\"The top {top_n_tokens} most common tokens\")\n",
    "        print(Counter(tokens).most_common(top_n_tokens))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "    \n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    return([t for t in tokens if t.lower() not in sw])\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    return text.lower().strip().split() \n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel fre to use the below cells as an example or read in the data in a way you prefer\n",
    "# change to your location if it is not in the same directory as your notebook\n",
    "data_location = \"C:/Users/zfreitas/Dropbox/Classes/USD/ADS-509-01-SP23 - Applied Text Mining/2. Module Two/Assignment 1/M1 Results/\" \n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df415d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"cher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "966804cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)\n",
    "\n",
    "twitter_data = twitter_data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "710e50de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hsmcnp</td>\n",
       "      <td>Country Girl</td>\n",
       "      <td>35152213</td>\n",
       "      <td></td>\n",
       "      <td>1302</td>\n",
       "      <td>1014</td>\n",
       "      <td></td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>742153090850164742</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81</td>\n",
       "      <td>514</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1496463006451974150</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3366479914</td>\n",
       "      <td></td>\n",
       "      <td>752</td>\n",
       "      <td>556</td>\n",
       "      <td>csu</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>83915043</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888</td>\n",
       "      <td>2891</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358367</th>\n",
       "      <td>jakegiles</td>\n",
       "      <td>Jake Giles</td>\n",
       "      <td>19725102</td>\n",
       "      <td>LA</td>\n",
       "      <td>7690</td>\n",
       "      <td>2165</td>\n",
       "      <td>singer of songs, type 1 diabetic, tired $jakel...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358368</th>\n",
       "      <td>axelbluhme</td>\n",
       "      <td>Axel Bluhme</td>\n",
       "      <td>19573759</td>\n",
       "      <td>DK</td>\n",
       "      <td>238</td>\n",
       "      <td>1565</td>\n",
       "      <td>Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358369</th>\n",
       "      <td>RayRayRay27</td>\n",
       "      <td>Rachael :)</td>\n",
       "      <td>19919217</td>\n",
       "      <td>Oldham</td>\n",
       "      <td>762</td>\n",
       "      <td>1479</td>\n",
       "      <td>A year to change a life is still a year ✨😌</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358370</th>\n",
       "      <td>bhandberg</td>\n",
       "      <td>Ben Handberg</td>\n",
       "      <td>12642462</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>432</td>\n",
       "      <td>593</td>\n",
       "      <td>Head of Consumer - Mango. Made in Melbourne. R...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358371</th>\n",
       "      <td>takemeback</td>\n",
       "      <td>Christine</td>\n",
       "      <td>15022058</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>182</td>\n",
       "      <td>260</td>\n",
       "      <td>Stand for what is right, even if you stand alone.</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4353175 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         screen_name          name                   id        location  \\\n",
       "0             hsmcnp  Country Girl             35152213                   \n",
       "1         horrormomy          Jeny   742153090850164742           Earth   \n",
       "2       anju79990584          anju  1496463006451974150                   \n",
       "3       gallionjenna             J           3366479914                   \n",
       "4            bcscomm       bcscomm             83915043  Washington, DC   \n",
       "...              ...           ...                  ...             ...   \n",
       "358367     jakegiles    Jake Giles             19725102              LA   \n",
       "358368    axelbluhme   Axel Bluhme             19573759              DK   \n",
       "358369   RayRayRay27    Rachael :)             19919217          Oldham   \n",
       "358370     bhandberg  Ben Handberg             12642462          Sydney   \n",
       "358371    takemeback     Christine             15022058      New Jersey   \n",
       "\n",
       "        followers_count  friends_count  \\\n",
       "0                  1302           1014   \n",
       "1                    81            514   \n",
       "2                    13            140   \n",
       "3                   752            556   \n",
       "4                   888           2891   \n",
       "...                 ...            ...   \n",
       "358367             7690           2165   \n",
       "358368              238           1565   \n",
       "358369              762           1479   \n",
       "358370              432            593   \n",
       "358371              182            260   \n",
       "\n",
       "                                              description artist  \n",
       "0                                                           cher  \n",
       "1                𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   cher  \n",
       "2               163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   cher  \n",
       "3                                                     csu   cher  \n",
       "4       Writer @Washinformer @SpelmanCollege alumna #D...   cher  \n",
       "...                                                   ...    ...  \n",
       "358367  singer of songs, type 1 diabetic, tired $jakel...  robyn  \n",
       "358368  Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...  robyn  \n",
       "358369         A year to change a life is still a year ✨😌  robyn  \n",
       "358370  Head of Consumer - Mango. Made in Melbourne. R...  robyn  \n",
       "358371  Stand for what is right, even if you stand alone.  robyn  \n",
       "\n",
       "[4353175 rows x 8 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3cb56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "# Dictionary Approach 2 - Using defaultdict\n",
    "# d[artist][title] = “the song lyrics as a string”\n",
    "lyrics = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "# Get the directory location and all the subfolders in directory.\n",
    "directory = data_location + lyrics_folder\n",
    "artist_folders = [name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "# Get all the files in each of the subfolders\n",
    "for artist in artist_folders:\n",
    "    artist_path = os.path.join(directory, artist)\n",
    "    for file in os.listdir(artist_path):\n",
    "        file_path = os.path.join(artist_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path) as f:\n",
    "                title = f.readline().strip()\n",
    "                lyrics[artist][title] = f.read().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "277efcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I light a candle in the morning \\nTo signify that your still on my mind\\nDarkness arrived without a warning \\nIt brought me down\\nBut I know the world just keeps on turning\\n\\nI wish that I could turn you on \\nLike a switch in my kitchen \\nRight before dawn\\n88 days seems so long\\nI believe in you and me\\nBut it's so hard to trust\\nSomething you just can't see, still I've got\\n\\n[CHORUS]\\n88 days 'til the sun\\nAnd while you're gone\\nI've got so much work inside my heart to be done, I've got\\n88 days 'til the sun\\nI`ve got to get my spirit ready\\nFor when the springtime comes\\n88 days 'til the sun\\n\\nZip up my thickest jacket \\nI miss the green and the light you gave to me\\nPrepare to get my feet wet\\nHalogen's on bright when 2 pm is like 2 in the night, it ain't right\\nSo what's the message in this song\\nThat the pain doesn't mean that you can't carry on\\nStill 88 days seem so long\\nA meditation, a revelation\\nBut it's so hard to trust\\nSomething you just can't see, still I've got\\n\\n[Chorus (x1)]\\n\\n88 days 'til the sun comes around\\n(You got work, you got work, you got work to be done)\\n88 days 'til the sun comes around\\n(You got work, you got work, you got work to be done)\\nA meditation, a revelation\\nBut it's so hard to trust\\nSomething you just can't see, I've got\\n\\nI've got 88 days, 88 days,\\n(You got work, you got work, you got work to be done)\\nI've got work, I've got work, I've got work to be done\\n(88 days 'til the sun)\\n88 days &lt;scat&gt;\\nGot to get my spirit ready for the springtime\\nStill I've got\\n(88 days 'til the sun)\\n[scat] springtime\\n(You got work, you got work, you got work to be done)\\n88 days 'til the springtime [fade out]\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check my work\n",
    "lyrics[\"robyn\"].get('\"88 Days\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6c5683a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>Stuck in L.A., ain't got no friends \\nAnd so H...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>What if the world was crazy and I was sane\\nWo...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>Well, here we are again\\nI guess it must be fa...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>Again evening finds me at your door \\nHere to ...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>What's it all about, Alfie?\\nIs it just for th...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>\"We Dance To The Beat\"</td>\n",
       "      <td>We dance to the beat\\nWe dance to the beat\\nWe...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>\"Where Did Our Love Go\"</td>\n",
       "      <td>Thoughts about you and me \\nThinkin' about wha...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>\"Who's That Girl\"</td>\n",
       "      <td>Good girls are pretty like all the time\\nI'm j...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>\"With Every Heartbeat\"</td>\n",
       "      <td>Maybe we could make it all right\\nWe could mak...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>\"You've Got That Something\"</td>\n",
       "      <td>Look at me here I am\\nI'm givin all of my lovi...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0                      \"88 Degrees\"   \n",
       "1   \"A Different Kind Of Love Song\"   \n",
       "2                       \"After All\"   \n",
       "3                           \"Again\"   \n",
       "4                           \"Alfie\"   \n",
       "..                              ...   \n",
       "88           \"We Dance To The Beat\"   \n",
       "89          \"Where Did Our Love Go\"   \n",
       "90                \"Who's That Girl\"   \n",
       "91           \"With Every Heartbeat\"   \n",
       "92      \"You've Got That Something\"   \n",
       "\n",
       "                                               lyrics artist  \n",
       "0   Stuck in L.A., ain't got no friends \\nAnd so H...   cher  \n",
       "1   What if the world was crazy and I was sane\\nWo...   cher  \n",
       "2   Well, here we are again\\nI guess it must be fa...   cher  \n",
       "3   Again evening finds me at your door \\nHere to ...   cher  \n",
       "4   What's it all about, Alfie?\\nIs it just for th...   cher  \n",
       "..                                                ...    ...  \n",
       "88  We dance to the beat\\nWe dance to the beat\\nWe...  robyn  \n",
       "89  Thoughts about you and me \\nThinkin' about wha...  robyn  \n",
       "90  Good girls are pretty like all the time\\nI'm j...  robyn  \n",
       "91  Maybe we could make it all right\\nWe could mak...  robyn  \n",
       "92  Look at me here I am\\nI'm givin all of my lovi...  robyn  \n",
       "\n",
       "[406 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create lyrics Pandas Dataframe for Cleaning\n",
    "artists = ['cher', 'robyn']\n",
    "lyrics_df = pd.DataFrame()\n",
    "\n",
    "for artist in artists:\n",
    "    lyrics_df_temp = pd.DataFrame(lyrics[artist].items(), columns=['title', 'lyrics'])\n",
    "    lyrics_df_temp['artist'] = artist\n",
    "    lyrics_df = pd.concat([lyrics_df, lyrics_df_temp])\n",
    "\n",
    "lyrics_data = lyrics_df.fillna('')\n",
    "\n",
    "lyrics_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare,pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>683690</th>\n",
       "      <td>cher</td>\n",
       "      <td>26♀ Black is my happy colour. Canto más que re...</td>\n",
       "      <td>[26♀, black, happy, colour, canto, más, que, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134925</th>\n",
       "      <td>cher</td>\n",
       "      <td>Licenciado en RRHH. Nada mas que decir señor j...</td>\n",
       "      <td>[licenciado, en, rrhh, nada, mas, que, decir, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513106</th>\n",
       "      <td>cher</td>\n",
       "      <td>M.D 💛 🌻✨ financially poor, spiritually rich ✨ ...</td>\n",
       "      <td>[md, 💛, 🌻✨, financially, poor, spiritually, ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417446</th>\n",
       "      <td>cher</td>\n",
       "      <td>Fur mummy 💝</td>\n",
       "      <td>[fur, mummy, 💝]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130640</th>\n",
       "      <td>cher</td>\n",
       "      <td>❤ 12-12-15, 10-13-16, 11-24-18, 11-25-18, 5-7-...</td>\n",
       "      <td>[❤, 121215, 101316, 112418, 112518, 5719, 5101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337866</th>\n",
       "      <td>cher</td>\n",
       "      <td>🇦🇲Proud Armenian✝️ 🇫🇷The UNS 🇫🇷UFAR</td>\n",
       "      <td>[🇦🇲proud, armenian✝️, 🇫🇷the, uns, 🇫🇷ufar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275663</th>\n",
       "      <td>cher</td>\n",
       "      <td>she/her I'm just that black chick you know tha...</td>\n",
       "      <td>[sheher, im, black, chick, know, curls, smilin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644060</th>\n",
       "      <td>cher</td>\n",
       "      <td>🌞Pisces 🌛Sagittarius ⬆️ Cancer</td>\n",
       "      <td>[🌞pisces, 🌛sagittarius, ⬆️, cancer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515017</th>\n",
       "      <td>cher</td>\n",
       "      <td>“Float like a butterfly, sting like a bee!” Ex...</td>\n",
       "      <td>[“float, like, butterfly, sting, like, bee”, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709966</th>\n",
       "      <td>cher</td>\n",
       "      <td>22 || Computer ScienceEngineer || Cricket 🏏</td>\n",
       "      <td>[22, computer, scienceengineer, cricket, 🏏]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description  \\\n",
       "683690    cher  26♀ Black is my happy colour. Canto más que re...   \n",
       "134925    cher  Licenciado en RRHH. Nada mas que decir señor j...   \n",
       "1513106   cher  M.D 💛 🌻✨ financially poor, spiritually rich ✨ ...   \n",
       "417446    cher                                        Fur mummy 💝   \n",
       "1130640   cher  ❤ 12-12-15, 10-13-16, 11-24-18, 11-25-18, 5-7-...   \n",
       "337866    cher                🇦🇲Proud Armenian✝️ 🇫🇷The UNS 🇫🇷UFAR   \n",
       "1275663   cher  she/her I'm just that black chick you know tha...   \n",
       "644060    cher                     🌞Pisces 🌛Sagittarius ⬆️ Cancer   \n",
       "1515017   cher  “Float like a butterfly, sting like a bee!” Ex...   \n",
       "3709966   cher        22 || Computer ScienceEngineer || Cricket 🏏   \n",
       "\n",
       "                                                    tokens  \n",
       "683690   [26♀, black, happy, colour, canto, más, que, r...  \n",
       "134925   [licenciado, en, rrhh, nada, mas, que, decir, ...  \n",
       "1513106  [md, 💛, 🌻✨, financially, poor, spiritually, ri...  \n",
       "417446                                     [fur, mummy, 💝]  \n",
       "1130640  [❤, 121215, 101316, 112418, 112518, 5719, 5101...  \n",
       "337866           [🇦🇲proud, armenian✝️, 🇫🇷the, uns, 🇫🇷ufar]  \n",
       "1275663  [sheher, im, black, chick, know, curls, smilin...  \n",
       "644060                 [🌞pisces, 🌛sagittarius, ⬆️, cancer]  \n",
       "1515017  [“float, like, butterfly, sting, like, bee”, e...  \n",
       "3709966        [22, computer, scienceengineer, cricket, 🏏]  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: I think there are several areas of improvement that could be made, especially on the twitter data. One is spelling correction. Removeal of numbers is another. This would be harder to do, cause some numbers have context assigned to them. Numbers like 9/11 or 911. It might also be nice to add the parts of speach that the tokens are in improve context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f1044a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls to descriptive_stats here\n",
    "\n",
    "# Helper Function\n",
    "def flatten_and_descriptive_stats(list_of_lists):\n",
    "    wordlist = [i for s in list_of_lists for i in s]\n",
    "    return descriptive_stats(wordlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d15021cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets for Cher:\n",
      "\n",
      "There are 34901 tokens in the data.\n",
      "There are 3682 unique tokens in the data.\n",
      "There are 167640 characters in the data.\n",
      "The lexical diversity is 0.105 in the data.\n",
      "The top 5 most common tokens\n",
      "[('love', 917), ('im', 510), ('know', 473), ('dont', 428), ('youre', 331)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[34901, 3682, 0.10549840978768517, 167640]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTweets for Cher:\\n\")\n",
    "flatten_and_descriptive_stats(lyrics_data.loc[lyrics_data['artist']=='cher'][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "87858ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets for Robyn:\n",
      "\n",
      "There are 13019 tokens in the data.\n",
      "There are 2139 unique tokens in the data.\n",
      "There are 62875 characters in the data.\n",
      "The lexical diversity is 0.164 in the data.\n",
      "The top 5 most common tokens\n",
      "[('im', 255), ('dont', 252), ('love', 238), ('know', 237), ('got', 230)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13019, 2139, 0.1642983332053153, 62875]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTweets for Robyn:\\n\")\n",
    "flatten_and_descriptive_stats(lyrics_data.loc[lyrics_data['artist']=='robyn'][\"tokens\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: Both Robyn and Cher lyrics have the same top 5 words. It is interesting to note that Robyn's lyrics have more lexical diversity than Cher's lyrics. It's interesting that this trend for lexical diversity follows for the two artist's tweets too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def token_normal(text): \n",
    "\n",
    "    # Lowercase and split on whitespace \n",
    "    text = text.lower().strip().split() \n",
    "\n",
    "    # Drop non—alpha and stopwords \n",
    "    text =  [w for w in text if w not in sw and w.isalpha()]\n",
    "\n",
    "    return(text) \n",
    "    \n",
    "\n",
    "def get_patterns(text, num_words):\n",
    "    \"\"\" \n",
    "    This function takes text as an input and returns a dictionary Of statistics, \n",
    "    after cleaning the text. \n",
    "    \"\"\"\n",
    "    if (len(text) == 0):\n",
    "        raise ValueError(\"Can't work with empty text object.\") \n",
    "\n",
    "\n",
    "    # We'll make things a big clearer by the \n",
    "    # statistics here. These are placeholder values. \n",
    "    total_tokens = 1\n",
    "    unique_tokens = 0\n",
    "    avg_token_len = 0.0\n",
    "    lex_diversity = 0.0\n",
    "    top_words =[]\n",
    "    \n",
    "    text = token_normal(text) \n",
    "\n",
    "    if len(text) == 0:\n",
    "        raise ValueError( \" All of text is stopwords! \" ) \n",
    "\n",
    "\n",
    "    # Calculate your statistics here \n",
    "    total_tokens = len(text) \n",
    "    unique_tokens = len(set(text)) \n",
    "    avg_token_len = np.mean([len(w) for w in text]) \n",
    "    lex_diversity = unique_tokens/total_tokens\n",
    "\n",
    "    top_words = FreqDist(text).most_common(num_words) \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    results = { 'tokens' : total_tokens, \n",
    "            'unique_tokens' : unique_tokens,\n",
    "            'avg_token_length' : avg_token_len,\n",
    "            'lexical_diversity': lex_diversity,\n",
    "            'top_words': top_words} \n",
    "\n",
    "    return(results) \n",
    "\n",
    "def get_word_frac(word, fd_corpus, length):  \n",
    "\n",
    "    if word in fd_corpus:\n",
    "        return(fd_corpus[word]/length) \n",
    "    else: \n",
    "        return(0)\n",
    "\n",
    "def get_ratio(word, fd_corpus_1, fd_corpus_2, len_1, len_2):\n",
    "\n",
    "    frac_1 = get_word_frac(word, fd_corpus_1, len_1) \n",
    "    frac_2 = get_word_frac(word, fd_corpus_2, len_2) \n",
    "\n",
    "    if frac_2 > 0:\n",
    "        return(frac_1/frac_2) \n",
    "    else:\n",
    "        return(float('NaN')) \n",
    "\n",
    "def compare_texts(corpus_1, corpus_2, num_words = 10, ratio_cutoff=5): \n",
    "    \"\"\"\n",
    "    This function returns a nested dictionary with information comparing two groups Of \n",
    "    text. See README for full description of what this function does. \n",
    "    \"\"\"\n",
    "    results = dict() \n",
    "\n",
    "    # Get the first two parts done with a function \n",
    "    results[\"one\"] = get_patterns(corpus_1, num_words)\n",
    "    results[\"two\"] = get_patterns(corpus_2, num_words)\n",
    "\n",
    "    # Now we start the ratio part. Cleaning first, then build \n",
    "    # frequency distributions \n",
    "    corpus_1 = token_normal(corpus_1) \n",
    "    corpus_2 =  token_normal(corpus_2)\n",
    "    \n",
    "    fd_1 = FreqDist(corpus_1) \n",
    "    fd_2 = FreqDist(corpus_2)\n",
    "\n",
    "    # It's handy to have a set of the words in each corpus. \n",
    "    \n",
    "    fd_1_words = set(fd_1.keys()) \n",
    "    fd_2_words = set(fd_2.keys()) \n",
    "    \n",
    "    # This will hold our ratios. Starting with 1 over 2 \n",
    "    holder = dict() \n",
    "    \n",
    "    # Also, we need to tell Python that the \"one_vs two\" spot holds \n",
    "    # a dictionary. (And \"two vs one\") \n",
    "    results[\"one_vs_two\"] = dict() \n",
    "    results[\"two_vs_one\"] = dict() \n",
    "    \n",
    "    # Now we add them. We check along the to make Sure \n",
    "    for word, count in fd_1.items():\n",
    "        if count > ratio_cutoff:\n",
    "            # This next line makes use of the fact that \n",
    "            # Python stops evaluating \"and\" expressions if it hits a False \n",
    "            if word in fd_2_words and fd_2[word] > ratio_cutoff:\n",
    "                holder[word] = get_ratio(word, fd_1, fd_2,\n",
    "                results[\"one\"][\"tokens\"],\n",
    "                results[\"two\"][\"tokens\"])\n",
    "    \n",
    "    num_added = 0\n",
    "\n",
    "    for word, frac in sorted(holder.items() , key=lambda item: -1*item[1]):\n",
    "        results[\"one_vs_two\"][word] = frac\n",
    "        num_added += 1\n",
    "        if num_added == num_words:\n",
    "            break \n",
    "\n",
    "    # Now we do the same for 2 vs 1!\n",
    "    holder = dict() \n",
    "    \n",
    "    # Now we add them. We check along the to make Sure \n",
    "    for word, count in fd_2.items():\n",
    "        if count > ratio_cutoff:\n",
    "            # This next line makes use of the fact that \n",
    "            # Python stops evaluating \"and\" expressions if it hits a False \n",
    "            if word in fd_1_words and fd_1[word] > ratio_cutoff:\n",
    "                holder[word] = get_ratio(word, fd_2, fd_1,\n",
    "                results[\"two\"][\"tokens\"],\n",
    "                results[\"one\"][\"tokens\"])\n",
    "    \n",
    "    num_added = 0    \n",
    "    \n",
    "    for word, frac in sorted(holder.items() , key=lambda item: -1*item[1]):\n",
    "        results[\"two_vs_one\"][word] = frac\n",
    "        num_added += 1\n",
    "        if num_added == num_words:\n",
    "            break \n",
    "    \n",
    "    return(results) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d74d8a4c",
   "metadata": {},
   "source": [
    "Frist let's look that the Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e7195d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = twitter_data.loc[twitter_data['artist']=='cher']['description']\n",
    "text2 = twitter_data.loc[twitter_data['artist']=='robyn']['description']\n",
    "compare_texts_results = compare_texts( \" \".join(text1) , \" \".join(text2), num_words=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c7a312f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 9941853,\n",
       " 'unique_tokens': 421383,\n",
       " 'avg_token_length': 5.546743147379065,\n",
       " 'lexical_diversity': 0.04238475463276313,\n",
       " 'top_words': [('love', 198887),\n",
       "  ('life', 86174),\n",
       "  ('de', 72776),\n",
       "  ('music', 60071),\n",
       "  ('follow', 59539),\n",
       "  ('like', 56907),\n",
       "  ('one', 41809),\n",
       "  ('live', 41108),\n",
       "  ('la', 39443),\n",
       "  ('im', 37431)]}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_results[\"one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e0cc8041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 955903,\n",
       " 'unique_tokens': 109164,\n",
       " 'avg_token_length': 5.612509846710388,\n",
       " 'lexical_diversity': 0.11419987174430879,\n",
       " 'top_words': [('love', 10465),\n",
       "  ('music', 10266),\n",
       "  ('och', 7895),\n",
       "  ('de', 6353),\n",
       "  ('follow', 5342),\n",
       "  ('life', 4984),\n",
       "  ('en', 4802),\n",
       "  ('like', 4738),\n",
       "  ('på', 4709),\n",
       "  ('new', 3521)]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_results[\"two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1a42a61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grandmother': 35.26759251016888,\n",
       " 'grandma': 20.96056479612,\n",
       " 'democrat': 11.548227295110019,\n",
       " 'trump': 11.098385553908885,\n",
       " 'nascar': 10.44823260479376,\n",
       " 'retired': 10.02090202791058,\n",
       " 'cowboys': 9.31275421478988,\n",
       " 'biden': 8.967532155893542,\n",
       " 'patriot': 8.589344595351927,\n",
       " 'grandson': 8.529823550714626}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_results[\"one_vs_two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f1cb94ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sveriges': 202.46273774640312,\n",
       " 'människor': 194.14235126367427,\n",
       " 'brinner': 193.15182906334937,\n",
       " 'följ': 187.20869586140017,\n",
       " 'spelar': 182.75134595993825,\n",
       " 'arbetar': 182.4812035416678,\n",
       " 'gärna': 175.76816444764793,\n",
       " 'försöker': 164.9219463540906,\n",
       " 'kommunikatör': 161.950379753116,\n",
       " 'stora': 149.07359114889275}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_results[\"two_vs_one\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1ce01f2",
   "metadata": {},
   "source": [
    "Now let's look at the lyrics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "63baec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = lyrics_data.loc[lyrics_data['artist']=='cher']['lyrics']\n",
    "text2 = lyrics_data.loc[lyrics_data['artist']=='robyn']['lyrics']\n",
    "compare_texts_lyrics_results = compare_texts( \" \".join(text1) , \" \".join(text2), num_words=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "076fff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 28271,\n",
       " 'unique_tokens': 3225,\n",
       " 'avg_token_length': 4.866152594531498,\n",
       " 'lexical_diversity': 0.11407449329701815,\n",
       " 'top_words': [('love', 851),\n",
       "  ('know', 441),\n",
       "  ('time', 299),\n",
       "  ('see', 284),\n",
       "  ('one', 267),\n",
       "  ('like', 256),\n",
       "  ('come', 248),\n",
       "  ('take', 246),\n",
       "  ('go', 246),\n",
       "  ('never', 242)]}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_lyrics_results[\"one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "248bccda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 10115,\n",
       " 'unique_tokens': 1820,\n",
       " 'avg_token_length': 4.914780029658923,\n",
       " 'lexical_diversity': 0.17993079584775087,\n",
       " 'top_words': [('know', 229),\n",
       "  ('got', 221),\n",
       "  ('love', 213),\n",
       "  ('like', 196),\n",
       "  ('baby', 169),\n",
       "  ('never', 136),\n",
       "  ('get', 127),\n",
       "  ('gonna', 105),\n",
       "  ('right', 104),\n",
       "  ('want', 102)]}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_lyrics_results[\"two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8c7765bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'find': 7.81168570855883,\n",
       " 'man': 7.672546268457273,\n",
       " 'believe': 7.453898576869112,\n",
       " 'enough': 5.903487672880337,\n",
       " 'us': 3.6375025055121273,\n",
       " 'well': 3.44776690537364,\n",
       " 'till': 3.2797153738224094,\n",
       " 'hope': 3.100821807977551,\n",
       " 'many': 3.041190619362598,\n",
       " 'door': 2.981559430747645}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_lyrics_results[\"one_vs_two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "99ef2597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beat': 13.229467787114848,\n",
       " 'work': 11.925154061624651,\n",
       " 'dance': 9.528265851795263,\n",
       " 'hang': 8.384873949579832,\n",
       " 'shake': 7.919047619047619,\n",
       " 'space': 6.638025210084034,\n",
       " 'moment': 6.388475390156062,\n",
       " 'alright': 5.869411764705883,\n",
       " 'control': 5.869411764705883,\n",
       " 'hurts': 5.124089635854341}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts_lyrics_results[\"two_vs_one\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: I noticed that Cher's tweets are more American centric in content, whereas Robyn had more Swedish unique words. I also noticed that Robyn's lyics may be more dance themed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b33cfd37f5195bd7836c1451c6eaacc84fbbad3c54541ec8bad2790bfb3f777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
